diff --git a/src/run_pipeline.py b/src/run_pipeline.py
index ed04c73..788df08 100644
--- a/src/run_pipeline.py
+++ b/src/run_pipeline.py
@@ -1,3 +1,5 @@
+#!/usr/bin/python
+
 import glob
 import os
 import random
@@ -12,6 +14,7 @@ from Bio.PDB.Polypeptide import index_to_one
 from torch.utils.data import DataLoader, Dataset
 
 import speedtest
+import argparse
 
 from rasp_model import (
     CavityModel,
@@ -69,39 +72,35 @@ EPOCHS_DS = 20
 NUM_ENSEMBLE = 10
 
 
-def main():
+def main(base_path, plot_vaex):
     # Pre-process all protein structures
     print(f"Pre-processing PDBs ...")
     pdb_dirs = [
-        f"{os.path.dirname(sys.path[0])}/data/train/cavity/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/train/downstream/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Rosetta_10/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/VAMP/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Xstal_vs_AF2/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/S669/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/structure/",
+        f"{base_path}/data/train/cavity/structure/",
+        f"{base_path}/data/train/downstream/structure/",
+        f"{base_path}/data/test/ProTherm/homology_models/structure/",
+        f"{base_path}/data/test/ProTherm/targets/structure/",
+        f"{base_path}/data/test/Protein_G/structure/",
+        f"{base_path}/data/test/Rosetta_10/structure/",
+        f"{base_path}/data/test/VAMP/structure/",
+        f"{base_path}/data/test/Xstal_vs_AF2/structure/",
+        f"{base_path}/data/test/S669/structure/",
+        f"{base_path}/data/test/Ssym_dir/structure/",
+        f"{base_path}/data/test/Ssym_inv/structure/",
+        f"{base_path}/data/test/Rocklin/structure/",
     ]
 
     for pdb_dir in pdb_dirs:
         subprocess.call(
             [
-                f"{os.path.dirname(sys.path[0])}/src/pdb_parser_scripts/parse_pdbs_pred.sh",
+                f"{os.path.dirname(__file__)}/pdb_parser_scripts/parse_pdbs_pred.sh",
                 str(pdb_dir),
             ]
         )
     print("Pre-processing finished.")
 
     # Load parsed PISCES PDBs and perform train/val split
-    pdb_filenames_cavity = sorted(
-        glob.glob(
-            f"{os.path.dirname(sys.path[0])}/data/train/cavity/structure/parsed/*coord*"
-        )
-    )
+    pdb_filenames_cavity = sorted(glob.glob(f"{base_path}/data/train/cavity/structure/parsed/*coord*"))
 
     if SHUFFLE_PDBS_CAVITY:
         random.shuffle(pdb_filenames_cavity)
@@ -111,16 +110,12 @@ def main():
         dataset_train_cavity,
         dataloader_val_cavity,
         dataset_val_cavity,
-    ) = train_val_split_cavity(
-        pdb_filenames_cavity, TRAIN_VAL_SPLIT_CAVITY, BATCH_SIZE_CAVITY, DEVICE
-    )
+    ) = train_val_split_cavity(pdb_filenames_cavity, TRAIN_VAL_SPLIT_CAVITY, BATCH_SIZE_CAVITY, DEVICE)
 
     # Define cavity model
     cavity_model_net = CavityModel(get_latent=False).to(DEVICE)
     loss_cavity = torch.nn.CrossEntropyLoss()
-    optimizer_cavity = torch.optim.Adam(
-        cavity_model_net.parameters(), lr=LEARNING_RATE_CAVITY
-    )
+    optimizer_cavity = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE_CAVITY)
 
     # Train cavity model
     print("Starting cavity model training")
@@ -136,24 +131,15 @@ def main():
     print("Finished cavity model training")
 
     # Create temporary residue environment datasets to more easily match ddG data
-    pdb_filenames_ds = sorted(
-        glob.glob(
-            f"{os.path.dirname(sys.path[0])}/data/train/downstream/structure/parsed/*coord*"
-        )
-    )
+    pdb_filenames_ds = sorted(glob.glob(f"{base_path}/data/train/downstream/structure/parsed/*coord*"))
     dataset = ResidueEnvironmentsDataset(pdb_filenames_ds)
     resenv_dataset = {}
     for resenv in dataset:
-        key = (
-            f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}"
-            f"{index_to_one(resenv.restype_index)}"
-        )
+        key = f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}" f"{index_to_one(resenv.restype_index)}"
         resenv_dataset[key] = resenv
 
     # Load ddG data to dataframe
-    df_ddg = pd.read_csv(
-        f"{os.path.dirname(sys.path[0])}/data/train/downstream/ddG_Rosetta/ddg.csv"
-    )
+    df_ddg = pd.read_csv(f"{base_path}/data/train/downstream/ddG_Rosetta/ddg.csv")
 
     # Populate dataframes with wt ResidueEnvironment objects and wt and mt restype indices
     populate_dfs_with_resenvs(df_ddg, resenv_dataset)
@@ -161,26 +147,22 @@ def main():
     # Remove disulfides
     n_df_start = len(df_ddg)
     df_ddg = remove_disulfides(df_ddg)
-    print(
-        f"{n_df_start-len(df_ddg)} data points removed due to disulfides in training and validation data."
-    )
+    print(f"{n_df_start-len(df_ddg)} data points removed due to disulfides in training and validation data.")
 
     # Do Fermi transform
     df_ddg["score_fermi"] = df_ddg["score"].apply(fermi_transform)
 
     # Checkpoint - save
-    df_ddg.to_pickle(f"{os.path.dirname(sys.path[0])}/output/ddg_train_val.pkl")
-    f = open(
-        f"{os.path.dirname(sys.path[0])}/output/cavity_models/best_model_path.txt", "w"
-    )
+    df_ddg.to_pickle(f"{base_path}/output/ddg_train_val.pkl")
+    f = open(f"{base_path}/output/cavity_models/best_model_path.txt", "w")
     f.write(best_cavity_model_path)
     f.close()
 
     # Checkpoint - load
-    df_ddg = pd.read_pickle(f"{os.path.dirname(sys.path[0])}/output/ddg_train_val.pkl")
+    df_ddg = pd.read_pickle(f"{base_path}/output/ddg_train_val.pkl")
     best_cavity_model_path = (
         open(
-            f"{os.path.dirname(sys.path[0])}/output/cavity_models/best_model_path.txt",
+            f"{base_path}/output/cavity_models/best_model_path.txt",
             "r",
         )
         .read()
@@ -188,15 +170,14 @@ def main():
     )
 
     # Split into train and val
-    dataloader_train_ds, dataloader_val_ds = train_val_split_ds(
-        df_ddg, BATCH_SIZE_DS, DEVICE
-    )
+    dataloader_train_ds, dataloader_val_ds = train_val_split_ds(df_ddg, BATCH_SIZE_DS, DEVICE)
 
     # Define model
     cavity_model_net = CavityModel(get_latent=True).to(DEVICE)
     cavity_model_net.load_state_dict(
-        torch.load(f"{os.path.dirname(sys.path[0])}/output/cavity_models/{best_cavity_model_path}",
-                   map_location=torch.device(DEVICE)
+        torch.load(
+            f"{base_path}/output/cavity_models/{best_cavity_model_path}",
+            map_location=torch.device(DEVICE),
         )
     )
     cavity_model_net.eval()
@@ -234,118 +215,42 @@ def main():
 
     # Load structure data
     pdb_filenames_test = {
-        "Rosetta": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Rosetta_10/structure/parsed/*coord*"
-            )
-        ),
-        "Protein_G": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/structure/parsed/*coord*"
-            )
-        ),
-        "ProTherm": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/structure/parsed/*coord*"
-            )
-        ),
-        "VAMP": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/VAMP/structure/parsed/*coord*"
-            )
-        ),
-        "Homology": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/structure/parsed/*coord*"
-            )
-        ),
-        "Xstal_vs_AF2": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Xstal_vs_AF2/structure/parsed/*coord*"
-            )
-        ),
-        "S669": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/S669/structure/parsed/*coord*"
-            )
-        ),
-        "Ssym_dir": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/structure/parsed/*coord*"
-            )
-        ),
-        "Ssym_inv": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/structure/parsed/*coord*"
-            )
-        ),
-        "Rocklin": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/structure/parsed/*coord*"
-            )
-        ),
+        "Rosetta": sorted(glob.glob(f"{base_path}/data/test/Rosetta_10/structure/parsed/*coord*")),
+        "Protein_G": sorted(glob.glob(f"{base_path}/data/test/Protein_G/structure/parsed/*coord*")),
+        "ProTherm": sorted(glob.glob(f"{base_path}/data/test/ProTherm/targets/structure/parsed/*coord*")),
+        "VAMP": sorted(glob.glob(f"{base_path}/data/test/VAMP/structure/parsed/*coord*")),
+        "Homology": sorted(glob.glob(f"{base_path}/data/test/ProTherm/homology_models/structure/parsed/*coord*")),
+        "Xstal_vs_AF2": sorted(glob.glob(f"{base_path}/data/test/Xstal_vs_AF2/structure/parsed/*coord*")),
+        "S669": sorted(glob.glob(f"{base_path}/data/test/S669/structure/parsed/*coord*")),
+        "Ssym_dir": sorted(glob.glob(f"{base_path}/data/test/Ssym_dir/structure/parsed/*coord*")),
+        "Ssym_inv": sorted(glob.glob(f"{base_path}/data/test/Ssym_inv/structure/parsed/*coord*")),
+        "Rocklin": sorted(glob.glob(f"{base_path}/data/test/Rocklin/structure/parsed/*coord*")),
     }
 
     # Load Rosetta ddGs
     rosetta_dict_test = {
-        "Rosetta": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Rosetta_10/ddG_Rosetta/ddg.csv"
-        ),
-        "Protein_G": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/ddG_Rosetta/ddg.csv"
-        ),
-        "ProTherm": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/ddG_Rosetta/ddg.csv"
-        ),
-        "VAMP": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/VAMP/ddG_Rosetta/ddg.csv"
-        ),
-        "Homology": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/ddG_Rosetta/ddg.csv"
-        ),
-        "Xstal_vs_AF2": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Xstal_vs_AF2/ddG_Rosetta/ddg.csv"
-        ),
-        "S669": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/S669/ddG_Rosetta/ddg.csv"
-         ),
-        "Ssym_dir": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/ddG_Rosetta/ddg.csv"
-         ),
-        "Ssym_inv": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/ddG_Rosetta/ddg.csv"
-         ),
-        "Rocklin": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/ddG_Rosetta/ddg.csv"
-         ),
+        "Rosetta": pd.read_csv(f"{base_path}/data/test/Rosetta_10/ddG_Rosetta/ddg.csv"),
+        "Protein_G": pd.read_csv(f"{base_path}/data/test/Protein_G/ddG_Rosetta/ddg.csv"),
+        "ProTherm": pd.read_csv(f"{base_path}/data/test/ProTherm/targets/ddG_Rosetta/ddg.csv"),
+        "VAMP": pd.read_csv(f"{base_path}/data/test/VAMP/ddG_Rosetta/ddg.csv"),
+        "Homology": pd.read_csv(f"{base_path}/data/test/ProTherm/homology_models/ddG_Rosetta/ddg.csv"),
+        "Xstal_vs_AF2": pd.read_csv(f"{base_path}/data/test/Xstal_vs_AF2/ddG_Rosetta/ddg.csv"),
+        "S669": pd.read_csv(f"{base_path}/data/test/S669/ddG_Rosetta/ddg.csv"),
+        "Ssym_dir": pd.read_csv(f"{base_path}/data/test/Ssym_dir/ddG_Rosetta/ddg.csv"),
+        "Ssym_inv": pd.read_csv(f"{base_path}/data/test/Ssym_inv/ddG_Rosetta/ddg.csv"),
+        "Rocklin": pd.read_csv(f"{base_path}/data/test/Rocklin/ddG_Rosetta/ddg.csv"),
     }
 
     # Load experimental ddGs and VAMP-seq score data
     exp_dict_test = {
-        "Protein_G": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/ddG_experimental/ddg.csv"
-        ),
-        "ProTherm": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/ddG_experimental/ddg.csv"
-        ),
-        "VAMP": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/VAMP/VAMP/VAMP.csv"
-        ),
-        "Homology": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/ddG_experimental/ddg.csv"
-        ),
-        "S669": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/S669/ddG_experimental/ddg.csv"
-        ),
-        "Ssym_dir": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/ddG_experimental/ddg.csv"
-        ),
-        "Ssym_inv": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/ddG_experimental/ddg.csv"
-        ),
-        "Rocklin": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/ddG_experimental/ddg.csv"
-        ),
+        "Protein_G": pd.read_csv(f"{base_path}/data/test/Protein_G/ddG_experimental/ddg.csv"),
+        "ProTherm": pd.read_csv(f"{base_path}/data/test/ProTherm/targets/ddG_experimental/ddg.csv"),
+        "VAMP": pd.read_csv(f"{base_path}/data/test/VAMP/VAMP/VAMP.csv"),
+        "Homology": pd.read_csv(f"{base_path}/data/test/ProTherm/homology_models/ddG_experimental/ddg.csv"),
+        "S669": pd.read_csv(f"{base_path}/data/test/S669/ddG_experimental/ddg.csv"),
+        "Ssym_dir": pd.read_csv(f"{base_path}/data/test/Ssym_dir/ddG_experimental/ddg.csv"),
+        "Ssym_inv": pd.read_csv(f"{base_path}/data/test/Ssym_inv/ddG_experimental/ddg.csv"),
+        "Rocklin": pd.read_csv(f"{base_path}/data/test/Rocklin/ddG_experimental/ddg.csv"),
     }
 
     # Match structure, Rosetta and experimental data
@@ -357,8 +262,7 @@ def main():
         resenv_dataset = {}
         for resenv in dataset_structure:
             key = (
-                f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}"
-                f"{index_to_one(resenv.restype_index)}"
+                f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}" f"{index_to_one(resenv.restype_index)}"
             )
             resenv_dataset[key] = resenv
 
@@ -378,15 +282,11 @@ def main():
             if dataset_key == "Homology":
                 df_rosetta["pdbid_target"] = df_rosetta["pdbid"].str[:4]
                 df_exp["pdbid_target"] = df_exp["pdbid"].str[:4]
-                df_rosetta = df_rosetta.merge(
-                    df_exp, on=["pdbid_target", "chainid", "variant"], how="inner"
-                )
+                df_rosetta = df_rosetta.merge(df_exp, on=["pdbid_target", "chainid", "variant"], how="inner")
                 df_rosetta["pdbid"] = df_rosetta["pdbid_x"]
                 df_rosetta.drop(["pdbid_x", "pdbid_y"], axis=1)
             else:
-                df_rosetta = df_rosetta.merge(
-                    df_exp, on=["pdbid", "chainid", "variant"], how="inner"
-                )
+                df_rosetta = df_rosetta.merge(df_exp, on=["pdbid", "chainid", "variant"], how="inner")
             print(
                 f"{len(df_exp)-len(df_rosetta)} data points dropped when (inner) matching Rosetta and structural data with experimental data in: {dataset_key} data set."
             )
@@ -396,14 +296,10 @@ def main():
         if dataset_key == "Rosetta":
             n_df_start = len(df_total)
             df_total = remove_disulfides(df_total)
-            print(
-                f"{n_df_start-len(df_total)} data points removed due to disulfides in: {dataset_key} data set."
-            )
+            print(f"{n_df_start-len(df_total)} data points removed due to disulfides in: {dataset_key} data set.")
 
         # Do Fermi transform
-        df_total["score_rosetta_fermi"] = df_total["score_rosetta"].apply(
-            fermi_transform
-        )
+        df_total["score_rosetta_fermi"] = df_total["score_rosetta"].apply(fermi_transform)
         if "score_exp" in df_total:
             df_total["score_exp_fermi"] = df_total["score_exp"].apply(fermi_transform)
 
@@ -420,15 +316,16 @@ def main():
     cavity_model_net = CavityModel(get_latent=True).to(DEVICE)
     best_cavity_model_path = (
         open(
-            f"{os.path.dirname(sys.path[0])}/output/cavity_models/best_model_path.txt",
+            f"{base_path}/output/cavity_models/best_model_path.txt",
             "r",
         )
         .read()
         .strip()
     )
     cavity_model_net.load_state_dict(
-        torch.load(f"{os.path.dirname(sys.path[0])}/output/cavity_models/{best_cavity_model_path}", 
-                   map_location=torch.device(DEVICE)
+        torch.load(
+            f"{base_path}/output/cavity_models/{best_cavity_model_path}",
+            map_location=torch.device(DEVICE),
         )
     )
     cavity_model_net.eval()
@@ -439,20 +336,16 @@ def main():
         print(f"Computing predictions for data set: {dataset_key}")
 
         # Compute predictions
-        df_ml = ds_pred(
-            cavity_model_net, ds_model_net, df_total, dataset_key, NUM_ENSEMBLE, DEVICE
-        )
-        
+        df_ml = ds_pred(cavity_model_net, ds_model_net, df_total, dataset_key, NUM_ENSEMBLE, DEVICE)
+
         # Merge and save data with predictions
-        df_total = df_total.merge(
-            df_ml, on=["pdbid", "chainid", "variant"], how="inner"
-        )
+        df_total = df_total.merge(df_ml, on=["pdbid", "chainid", "variant"], how="inner")
         if dataset_key != "Homology":
             print(
                 f"{len(df_ml)-len(df_total)} data points dropped when (inner) matching total data with ml predictions in: {dataset_key} data set."
             )
         df_total.to_pickle(
-            f"{os.path.dirname(sys.path[0])}/output/{dataset_key}/ml_preds.pkl",
+            f"{base_path}/output/{dataset_key}/ml_preds.pkl",
         )
 
         # Make visualizations
@@ -469,13 +362,24 @@ def main():
     speedtest.run()
 
     # Plot large cytosolic data set vs gnomAD and ClinVar
-    plot_gnomad_clinvar(xlim=[-1,7])
-    plot_gnomad_clinvar(xlim=[-4,14])
+    plot_gnomad_clinvar(xlim=[-1, 7])
+    plot_gnomad_clinvar(xlim=[-4, 14])
     bootstrap_gnomad_clinvar()
 
     # Plot human proteome data set
-    df = vaex.open(f"{os.path.dirname(sys.path[0])}/data/test/Human/data/all_rasp_preds_vaex_dataframe.hdf5")
-    hist_plot_all(df, "Human", 6)
+    if plot_vaex:
+        df = vaex.open(f"{base_path}/data/test/Human/data/all_rasp_preds_vaex_dataframe.hdf5")
+        hist_plot_all(df, "Human", 6)
+
 
 if __name__ == "__main__":
-    main()
+    parser = argparse.ArgumentParser(description="Running RaSP-model.")
+    parser.add_argument(
+        "--path",
+        type=str,
+        default=".",
+        help="set path to the folder that contains data and output subdirectories; default: . (current directory)",
+    )
+    parser.add_argument("--disable_vaex_plot", action="store_false", help="Define whether the vaex plot is disabled)")
+    args = parser.parse_args()
+    main(args.path, args.disable_vaex_plot)
