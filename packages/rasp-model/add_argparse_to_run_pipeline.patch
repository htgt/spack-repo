diff --git a/src/pdb_parser_scripts/extract_environments.py b/src/pdb_parser_scripts/extract_environments.py
index f569753..62f3df0 100644
--- a/src/pdb_parser_scripts/extract_environments.py
+++ b/src/pdb_parser_scripts/extract_environments.py
@@ -47,9 +47,7 @@ def extract_atomic_features(pdb_filename):
             b_factors_atom = []
             for atom in res:
                 b_factors_atom.append(atom.get_bfactor())
-            b_factors_res.append(
-                sum(b_factors_atom) / len(b_factors_atom)
-            )  # Take mean of atomic B factors
+            b_factors_res.append(sum(b_factors_atom) / len(b_factors_atom))  # Take mean of atomic B factors
 
         # Add to global container for this protein
         sequence.append(sequence_chain)
@@ -91,9 +89,7 @@ def extract_atomic_features(pdb_filename):
     # Save features in a dictionary
     features = {}
     features["atom_names"] = []  # Atom names
-    features[
-        "res_indices"
-    ] = []  # Global res indices (no reset across chains and starts from 0)
+    features["res_indices"] = []  # Global res indices (no reset across chains and starts from 0)
     features["resids_pdb"] = []  # Resids in the actual odbs
     features["x"] = []
     features["y"] = []
@@ -104,7 +100,6 @@ def extract_atomic_features(pdb_filename):
         chain_start_index = chain_boundary_indices[i]
         for j, residue in enumerate(chain.residues()):
             for atom in residue.atoms():
-
                 # Extract atom features
                 index = atom.index
                 position = list(positions[index].value_in_unit(simtk.unit.angstrom))
@@ -121,7 +116,7 @@ def extract_atomic_features(pdb_filename):
     # Convert valid lists to numpy arrays
     # (even convert atom_names since its simpler to mask with despite being str)
     features["atom_names"] = np.array(features["atom_names"], dtype="a5")
-    features["res_indices"] = np.array(features["res_indices"], dtype=np.int)
+    features["res_indices"] = np.array(features["res_indices"], dtype=np.int32)
     features["x"] = np.array(features["x"], dtype=np.float32)
     features["y"] = np.array(features["y"], dtype=np.float32)
     features["z"] = np.array(features["z"], dtype=np.float32)
@@ -148,9 +143,7 @@ def extract_coordinates(features, max_radius, include_center):
     # not residues. It counts globally across chains, i.e. no resets and
     # starts from zero.
     res_indices_glob = features["res_indices"]
-    res_indices_uniq = np.unique(
-        res_indices_glob
-    )  # Has length == number of total residues
+    res_indices_uniq = np.unique(res_indices_glob)  # Has length == number of total residues
 
     # Begin
     selector_list = []
@@ -159,26 +152,13 @@ def extract_coordinates(features, max_radius, include_center):
     for residue_index in res_indices_uniq:
         # Extract origin mask
         if (
-            np.logical_and(
-                res_indices_glob == residue_index, features["atom_names"] == b"N"
-            ).any()
-            and np.logical_and(
-                res_indices_glob == residue_index, features["atom_names"] == b"CA"
-            ).any()
-            and np.logical_and(
-                res_indices_glob == residue_index, features["atom_names"] == b"C"
-            ).any()
+            np.logical_and(res_indices_glob == residue_index, features["atom_names"] == b"N").any()
+            and np.logical_and(res_indices_glob == residue_index, features["atom_names"] == b"CA").any()
+            and np.logical_and(res_indices_glob == residue_index, features["atom_names"] == b"C").any()
         ):
-
-            N_mask = np.logical_and(
-                res_indices_glob == residue_index, features["atom_names"] == b"N"
-            )
-            CA_mask = np.logical_and(
-                res_indices_glob == residue_index, features["atom_names"] == b"CA"
-            )
-            C_mask = np.logical_and(
-                res_indices_glob == residue_index, features["atom_names"] == b"C"
-            )
+            N_mask = np.logical_and(res_indices_glob == residue_index, features["atom_names"] == b"N")
+            CA_mask = np.logical_and(res_indices_glob == residue_index, features["atom_names"] == b"CA")
+            C_mask = np.logical_and(res_indices_glob == residue_index, features["atom_names"] == b"C")
         else:
             # Store None to maintain indices
             indices_list.append(None)
@@ -186,15 +166,9 @@ def extract_coordinates(features, max_radius, include_center):
             continue
 
         # Extract origin
-        pos_N = np.array(
-            [features["x"][N_mask], features["y"][N_mask], features["z"][N_mask]]
-        ).squeeze()
-        pos_CA = np.array(
-            [features["x"][CA_mask], features["y"][CA_mask], features["z"][CA_mask]]
-        ).squeeze()
-        pos_C = np.array(
-            [features["x"][C_mask], features["y"][C_mask], features["z"][C_mask]]
-        ).squeeze()
+        pos_N = np.array([features["x"][N_mask], features["y"][N_mask], features["z"][N_mask]]).squeeze()
+        pos_CA = np.array([features["x"][CA_mask], features["y"][CA_mask], features["z"][CA_mask]]).squeeze()
+        pos_C = np.array([features["x"][C_mask], features["y"][C_mask], features["z"][C_mask]]).squeeze()
 
         # Define Cartesian coordinate system
         coordinate_system = grid.CoordinateSystem["cartesian"]
@@ -216,17 +190,13 @@ def extract_coordinates(features, max_radius, include_center):
             selector = np.where(r < max_radius)[0]
         else:
             # ALSO exclude features from residue itself
-            selector = np.where(
-                np.logical_and(r < max_radius, res_indices_glob != residue_index)
-            )[0]
+            selector = np.where(np.logical_and(r < max_radius, res_indices_glob != residue_index))[0]
 
         xyz_ref_origo_list.append(xyz[selector])
         selector_list.append(selector)
 
     # Find max number of atoms in an environment with radias = max_radius
-    max_selector = max(
-        [len(selector) for selector in selector_list if selector is not None]
-    )
+    max_selector = max([len(selector) for selector in selector_list if selector is not None])
     selector_array = np.full((len(selector_list), max_selector), -1, dtype=np.int32)
     for i, selector in enumerate(selector_list):
         if selector is not None:
@@ -289,9 +259,7 @@ def extract_environments(
 
     # Extract relevant coordinates (already masked with selector and referenced),
     # all atom types and the mask for each residue (selector_array)
-    xyz_ref_origo_arr, atom_types_numeric, selector_array = extract_coordinates(
-        features, max_radius, include_center
-    )
+    xyz_ref_origo_arr, atom_types_numeric, selector_array = extract_coordinates(features, max_radius, include_center)
 
     # Save as .npz
     np.savez_compressed(
@@ -320,7 +288,6 @@ def str2bool(v):
 
 
 if __name__ == "__main__":
-
     t0 = time.time()
 
     # Argument parser
diff --git a/src/run_pipeline.py b/src/run_pipeline.py
index ed04c73..3a3e407 100644
--- a/src/run_pipeline.py
+++ b/src/run_pipeline.py
@@ -1,3 +1,5 @@
+#!/usr/bin/python
+
 import glob
 import os
 import random
@@ -12,6 +14,7 @@ from Bio.PDB.Polypeptide import index_to_one
 from torch.utils.data import DataLoader, Dataset
 
 import speedtest
+import argparse
 
 from rasp_model import (
     CavityModel,
@@ -69,39 +72,35 @@ EPOCHS_DS = 20
 NUM_ENSEMBLE = 10
 
 
-def main():
+def main(base_path, plot_vaex):
     # Pre-process all protein structures
     print(f"Pre-processing PDBs ...")
     pdb_dirs = [
-        f"{os.path.dirname(sys.path[0])}/data/train/cavity/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/train/downstream/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Rosetta_10/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/VAMP/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Xstal_vs_AF2/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/S669/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/structure/",
-        f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/structure/",
+        f"{base_path}/data/train/cavity/structure/",
+        f"{base_path}/data/train/downstream/structure/",
+        f"{base_path}/data/test/ProTherm/homology_models/structure/",
+        f"{base_path}/data/test/ProTherm/targets/structure/",
+        f"{base_path}/data/test/Protein_G/structure/",
+        f"{base_path}/data/test/Rosetta_10/structure/",
+        f"{base_path}/data/test/VAMP/structure/",
+        f"{base_path}/data/test/Xstal_vs_AF2/structure/",
+        f"{base_path}/data/test/S669/structure/",
+        f"{base_path}/data/test/Ssym_dir/structure/",
+        f"{base_path}/data/test/Ssym_inv/structure/",
+        f"{base_path}/data/test/Rocklin/structure/",
     ]
 
     for pdb_dir in pdb_dirs:
         subprocess.call(
             [
-                f"{os.path.dirname(sys.path[0])}/src/pdb_parser_scripts/parse_pdbs_pred.sh",
+                f"{os.path.dirname(__file__)}/pdb_parser_scripts/parse_pdbs_pred.sh",
                 str(pdb_dir),
             ]
         )
     print("Pre-processing finished.")
 
     # Load parsed PISCES PDBs and perform train/val split
-    pdb_filenames_cavity = sorted(
-        glob.glob(
-            f"{os.path.dirname(sys.path[0])}/data/train/cavity/structure/parsed/*coord*"
-        )
-    )
+    pdb_filenames_cavity = sorted(glob.glob(f"{base_path}/data/train/cavity/structure/parsed/*coord*"))
 
     if SHUFFLE_PDBS_CAVITY:
         random.shuffle(pdb_filenames_cavity)
@@ -111,16 +110,12 @@ def main():
         dataset_train_cavity,
         dataloader_val_cavity,
         dataset_val_cavity,
-    ) = train_val_split_cavity(
-        pdb_filenames_cavity, TRAIN_VAL_SPLIT_CAVITY, BATCH_SIZE_CAVITY, DEVICE
-    )
+    ) = train_val_split_cavity(pdb_filenames_cavity, TRAIN_VAL_SPLIT_CAVITY, BATCH_SIZE_CAVITY, DEVICE)
 
     # Define cavity model
     cavity_model_net = CavityModel(get_latent=False).to(DEVICE)
     loss_cavity = torch.nn.CrossEntropyLoss()
-    optimizer_cavity = torch.optim.Adam(
-        cavity_model_net.parameters(), lr=LEARNING_RATE_CAVITY
-    )
+    optimizer_cavity = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE_CAVITY)
 
     # Train cavity model
     print("Starting cavity model training")
@@ -136,24 +131,15 @@ def main():
     print("Finished cavity model training")
 
     # Create temporary residue environment datasets to more easily match ddG data
-    pdb_filenames_ds = sorted(
-        glob.glob(
-            f"{os.path.dirname(sys.path[0])}/data/train/downstream/structure/parsed/*coord*"
-        )
-    )
+    pdb_filenames_ds = sorted(glob.glob(f"{base_path}/data/train/downstream/structure/parsed/*coord*"))
     dataset = ResidueEnvironmentsDataset(pdb_filenames_ds)
     resenv_dataset = {}
     for resenv in dataset:
-        key = (
-            f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}"
-            f"{index_to_one(resenv.restype_index)}"
-        )
+        key = f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}" f"{index_to_one(resenv.restype_index)}"
         resenv_dataset[key] = resenv
 
     # Load ddG data to dataframe
-    df_ddg = pd.read_csv(
-        f"{os.path.dirname(sys.path[0])}/data/train/downstream/ddG_Rosetta/ddg.csv"
-    )
+    df_ddg = pd.read_csv(f"{base_path}/data/train/downstream/ddG_Rosetta/ddg.csv")
 
     # Populate dataframes with wt ResidueEnvironment objects and wt and mt restype indices
     populate_dfs_with_resenvs(df_ddg, resenv_dataset)
@@ -161,26 +147,22 @@ def main():
     # Remove disulfides
     n_df_start = len(df_ddg)
     df_ddg = remove_disulfides(df_ddg)
-    print(
-        f"{n_df_start-len(df_ddg)} data points removed due to disulfides in training and validation data."
-    )
+    print(f"{n_df_start-len(df_ddg)} data points removed due to disulfides in training and validation data.")
 
     # Do Fermi transform
     df_ddg["score_fermi"] = df_ddg["score"].apply(fermi_transform)
 
     # Checkpoint - save
-    df_ddg.to_pickle(f"{os.path.dirname(sys.path[0])}/output/ddg_train_val.pkl")
-    f = open(
-        f"{os.path.dirname(sys.path[0])}/output/cavity_models/best_model_path.txt", "w"
-    )
+    df_ddg.to_pickle(f"{base_path}/output/ddg_train_val.pkl")
+    f = open(f"{base_path}/output/cavity_models/best_model_path.txt", "w")
     f.write(best_cavity_model_path)
     f.close()
 
     # Checkpoint - load
-    df_ddg = pd.read_pickle(f"{os.path.dirname(sys.path[0])}/output/ddg_train_val.pkl")
+    df_ddg = pd.read_pickle(f"{base_path}/output/ddg_train_val.pkl")
     best_cavity_model_path = (
         open(
-            f"{os.path.dirname(sys.path[0])}/output/cavity_models/best_model_path.txt",
+            f"{base_path}/output/cavity_models/best_model_path.txt",
             "r",
         )
         .read()
@@ -188,15 +170,14 @@ def main():
     )
 
     # Split into train and val
-    dataloader_train_ds, dataloader_val_ds = train_val_split_ds(
-        df_ddg, BATCH_SIZE_DS, DEVICE
-    )
+    dataloader_train_ds, dataloader_val_ds = train_val_split_ds(df_ddg, BATCH_SIZE_DS, DEVICE)
 
     # Define model
     cavity_model_net = CavityModel(get_latent=True).to(DEVICE)
     cavity_model_net.load_state_dict(
-        torch.load(f"{os.path.dirname(sys.path[0])}/output/cavity_models/{best_cavity_model_path}",
-                   map_location=torch.device(DEVICE)
+        torch.load(
+            f"{base_path}/output/cavity_models/{best_cavity_model_path}",
+            map_location=torch.device(DEVICE),
         )
     )
     cavity_model_net.eval()
@@ -234,118 +215,42 @@ def main():
 
     # Load structure data
     pdb_filenames_test = {
-        "Rosetta": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Rosetta_10/structure/parsed/*coord*"
-            )
-        ),
-        "Protein_G": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/structure/parsed/*coord*"
-            )
-        ),
-        "ProTherm": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/structure/parsed/*coord*"
-            )
-        ),
-        "VAMP": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/VAMP/structure/parsed/*coord*"
-            )
-        ),
-        "Homology": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/structure/parsed/*coord*"
-            )
-        ),
-        "Xstal_vs_AF2": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Xstal_vs_AF2/structure/parsed/*coord*"
-            )
-        ),
-        "S669": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/S669/structure/parsed/*coord*"
-            )
-        ),
-        "Ssym_dir": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/structure/parsed/*coord*"
-            )
-        ),
-        "Ssym_inv": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/structure/parsed/*coord*"
-            )
-        ),
-        "Rocklin": sorted(
-            glob.glob(
-                f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/structure/parsed/*coord*"
-            )
-        ),
+        "Rosetta": sorted(glob.glob(f"{base_path}/data/test/Rosetta_10/structure/parsed/*coord*")),
+        "Protein_G": sorted(glob.glob(f"{base_path}/data/test/Protein_G/structure/parsed/*coord*")),
+        "ProTherm": sorted(glob.glob(f"{base_path}/data/test/ProTherm/targets/structure/parsed/*coord*")),
+        "VAMP": sorted(glob.glob(f"{base_path}/data/test/VAMP/structure/parsed/*coord*")),
+        "Homology": sorted(glob.glob(f"{base_path}/data/test/ProTherm/homology_models/structure/parsed/*coord*")),
+        "Xstal_vs_AF2": sorted(glob.glob(f"{base_path}/data/test/Xstal_vs_AF2/structure/parsed/*coord*")),
+        "S669": sorted(glob.glob(f"{base_path}/data/test/S669/structure/parsed/*coord*")),
+        "Ssym_dir": sorted(glob.glob(f"{base_path}/data/test/Ssym_dir/structure/parsed/*coord*")),
+        "Ssym_inv": sorted(glob.glob(f"{base_path}/data/test/Ssym_inv/structure/parsed/*coord*")),
+        "Rocklin": sorted(glob.glob(f"{base_path}/data/test/Rocklin/structure/parsed/*coord*")),
     }
 
     # Load Rosetta ddGs
     rosetta_dict_test = {
-        "Rosetta": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Rosetta_10/ddG_Rosetta/ddg.csv"
-        ),
-        "Protein_G": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/ddG_Rosetta/ddg.csv"
-        ),
-        "ProTherm": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/ddG_Rosetta/ddg.csv"
-        ),
-        "VAMP": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/VAMP/ddG_Rosetta/ddg.csv"
-        ),
-        "Homology": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/ddG_Rosetta/ddg.csv"
-        ),
-        "Xstal_vs_AF2": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Xstal_vs_AF2/ddG_Rosetta/ddg.csv"
-        ),
-        "S669": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/S669/ddG_Rosetta/ddg.csv"
-         ),
-        "Ssym_dir": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/ddG_Rosetta/ddg.csv"
-         ),
-        "Ssym_inv": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/ddG_Rosetta/ddg.csv"
-         ),
-        "Rocklin": pd.read_csv(
-             f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/ddG_Rosetta/ddg.csv"
-         ),
+        "Rosetta": pd.read_csv(f"{base_path}/data/test/Rosetta_10/ddG_Rosetta/ddg.csv"),
+        "Protein_G": pd.read_csv(f"{base_path}/data/test/Protein_G/ddG_Rosetta/ddg.csv"),
+        "ProTherm": pd.read_csv(f"{base_path}/data/test/ProTherm/targets/ddG_Rosetta/ddg.csv"),
+        "VAMP": pd.read_csv(f"{base_path}/data/test/VAMP/ddG_Rosetta/ddg.csv"),
+        "Homology": pd.read_csv(f"{base_path}/data/test/ProTherm/homology_models/ddG_Rosetta/ddg.csv"),
+        "Xstal_vs_AF2": pd.read_csv(f"{base_path}/data/test/Xstal_vs_AF2/ddG_Rosetta/ddg.csv"),
+        "S669": pd.read_csv(f"{base_path}/data/test/S669/ddG_Rosetta/ddg.csv"),
+        "Ssym_dir": pd.read_csv(f"{base_path}/data/test/Ssym_dir/ddG_Rosetta/ddg.csv"),
+        "Ssym_inv": pd.read_csv(f"{base_path}/data/test/Ssym_inv/ddG_Rosetta/ddg.csv"),
+        "Rocklin": pd.read_csv(f"{base_path}/data/test/Rocklin/ddG_Rosetta/ddg.csv"),
     }
 
     # Load experimental ddGs and VAMP-seq score data
     exp_dict_test = {
-        "Protein_G": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/Protein_G/ddG_experimental/ddg.csv"
-        ),
-        "ProTherm": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/targets/ddG_experimental/ddg.csv"
-        ),
-        "VAMP": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/VAMP/VAMP/VAMP.csv"
-        ),
-        "Homology": pd.read_csv(
-            f"{os.path.dirname(sys.path[0])}/data/test/ProTherm/homology_models/ddG_experimental/ddg.csv"
-        ),
-        "S669": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/S669/ddG_experimental/ddg.csv"
-        ),
-        "Ssym_dir": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_dir/ddG_experimental/ddg.csv"
-        ),
-        "Ssym_inv": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/Ssym_inv/ddG_experimental/ddg.csv"
-        ),
-        "Rocklin": pd.read_csv(
-                f"{os.path.dirname(sys.path[0])}/data/test/Rocklin/ddG_experimental/ddg.csv"
-        ),
+        "Protein_G": pd.read_csv(f"{base_path}/data/test/Protein_G/ddG_experimental/ddg.csv"),
+        "ProTherm": pd.read_csv(f"{base_path}/data/test/ProTherm/targets/ddG_experimental/ddg.csv"),
+        "VAMP": pd.read_csv(f"{base_path}/data/test/VAMP/VAMP/VAMP.csv"),
+        "Homology": pd.read_csv(f"{base_path}/data/test/ProTherm/homology_models/ddG_experimental/ddg.csv"),
+        "S669": pd.read_csv(f"{base_path}/data/test/S669/ddG_experimental/ddg.csv"),
+        "Ssym_dir": pd.read_csv(f"{base_path}/data/test/Ssym_dir/ddG_experimental/ddg.csv"),
+        "Ssym_inv": pd.read_csv(f"{base_path}/data/test/Ssym_inv/ddG_experimental/ddg.csv"),
+        "Rocklin": pd.read_csv(f"{base_path}/data/test/Rocklin/ddG_experimental/ddg.csv"),
     }
 
     # Match structure, Rosetta and experimental data
@@ -357,8 +262,7 @@ def main():
         resenv_dataset = {}
         for resenv in dataset_structure:
             key = (
-                f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}"
-                f"{index_to_one(resenv.restype_index)}"
+                f"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}" f"{index_to_one(resenv.restype_index)}"
             )
             resenv_dataset[key] = resenv
 
@@ -378,15 +282,11 @@ def main():
             if dataset_key == "Homology":
                 df_rosetta["pdbid_target"] = df_rosetta["pdbid"].str[:4]
                 df_exp["pdbid_target"] = df_exp["pdbid"].str[:4]
-                df_rosetta = df_rosetta.merge(
-                    df_exp, on=["pdbid_target", "chainid", "variant"], how="inner"
-                )
+                df_rosetta = df_rosetta.merge(df_exp, on=["pdbid_target", "chainid", "variant"], how="inner")
                 df_rosetta["pdbid"] = df_rosetta["pdbid_x"]
                 df_rosetta.drop(["pdbid_x", "pdbid_y"], axis=1)
             else:
-                df_rosetta = df_rosetta.merge(
-                    df_exp, on=["pdbid", "chainid", "variant"], how="inner"
-                )
+                df_rosetta = df_rosetta.merge(df_exp, on=["pdbid", "chainid", "variant"], how="inner")
             print(
                 f"{len(df_exp)-len(df_rosetta)} data points dropped when (inner) matching Rosetta and structural data with experimental data in: {dataset_key} data set."
             )
@@ -396,14 +296,10 @@ def main():
         if dataset_key == "Rosetta":
             n_df_start = len(df_total)
             df_total = remove_disulfides(df_total)
-            print(
-                f"{n_df_start-len(df_total)} data points removed due to disulfides in: {dataset_key} data set."
-            )
+            print(f"{n_df_start-len(df_total)} data points removed due to disulfides in: {dataset_key} data set.")
 
         # Do Fermi transform
-        df_total["score_rosetta_fermi"] = df_total["score_rosetta"].apply(
-            fermi_transform
-        )
+        df_total["score_rosetta_fermi"] = df_total["score_rosetta"].apply(fermi_transform)
         if "score_exp" in df_total:
             df_total["score_exp_fermi"] = df_total["score_exp"].apply(fermi_transform)
 
@@ -420,15 +316,16 @@ def main():
     cavity_model_net = CavityModel(get_latent=True).to(DEVICE)
     best_cavity_model_path = (
         open(
-            f"{os.path.dirname(sys.path[0])}/output/cavity_models/best_model_path.txt",
+            f"{base_path}/output/cavity_models/best_model_path.txt",
             "r",
         )
         .read()
         .strip()
     )
     cavity_model_net.load_state_dict(
-        torch.load(f"{os.path.dirname(sys.path[0])}/output/cavity_models/{best_cavity_model_path}", 
-                   map_location=torch.device(DEVICE)
+        torch.load(
+            f"{base_path}/output/cavity_models/{best_cavity_model_path}",
+            map_location=torch.device(DEVICE),
         )
     )
     cavity_model_net.eval()
@@ -439,20 +336,16 @@ def main():
         print(f"Computing predictions for data set: {dataset_key}")
 
         # Compute predictions
-        df_ml = ds_pred(
-            cavity_model_net, ds_model_net, df_total, dataset_key, NUM_ENSEMBLE, DEVICE
-        )
-        
+        df_ml = ds_pred(cavity_model_net, ds_model_net, df_total, dataset_key, NUM_ENSEMBLE, DEVICE)
+
         # Merge and save data with predictions
-        df_total = df_total.merge(
-            df_ml, on=["pdbid", "chainid", "variant"], how="inner"
-        )
+        df_total = df_total.merge(df_ml, on=["pdbid", "chainid", "variant"], how="inner")
         if dataset_key != "Homology":
             print(
                 f"{len(df_ml)-len(df_total)} data points dropped when (inner) matching total data with ml predictions in: {dataset_key} data set."
             )
         df_total.to_pickle(
-            f"{os.path.dirname(sys.path[0])}/output/{dataset_key}/ml_preds.pkl",
+            f"{base_path}/output/{dataset_key}/ml_preds.pkl",
         )
 
         # Make visualizations
@@ -469,13 +362,25 @@ def main():
     speedtest.run()
 
     # Plot large cytosolic data set vs gnomAD and ClinVar
-    plot_gnomad_clinvar(xlim=[-1,7])
-    plot_gnomad_clinvar(xlim=[-4,14])
+    plot_gnomad_clinvar(xlim=[-1, 7])
+    plot_gnomad_clinvar(xlim=[-4, 14])
     bootstrap_gnomad_clinvar()
 
     # Plot human proteome data set
-    df = vaex.open(f"{os.path.dirname(sys.path[0])}/data/test/Human/data/all_rasp_preds_vaex_dataframe.hdf5")
-    hist_plot_all(df, "Human", 6)
+    if plot_vaex:
+        df = vaex.open(f"{base_path}/data/test/Human/data/all_rasp_preds_vaex_dataframe.hdf5")
+        hist_plot_all(df, "Human", 6)
+
 
 if __name__ == "__main__":
-    main()
+    parser = argparse.ArgumentParser(description="Running RaSP-model.")
+    parser.add_argument(
+        "--path",
+        type=str,
+        default=".",
+        help="set path to the folder that contains data and output subdirectories; default: . (current directory)",
+    )
+    parser.add_argument("--disable_vaex_plot", action="store_false", help="Define whether the vaex plot is disabled)")
+    args = parser.parse_args()
+    os.environ["RASP_PATH"] = args.path
+    main(args.path, args.disable_vaex_plot)
